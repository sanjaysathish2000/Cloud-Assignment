{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjaysathish2000/Cloud-Assignment/blob/main/Spam_vs_Ham.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "id": "r4teBNmlNtaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "232YKq84Ne-u"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 1: Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"Spam vs Ham\").getOrCreate()\n",
        "\n",
        "# Step 2: Read a dataset\n",
        "file_path = \"/content/spam_email_dataset.csv\"\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "# You can adjust options based on your CSV file's format\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Step 3: Perform operations on the DataFrame\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import col, lower, when\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import lit, explode\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
        "from math import log\n",
        "from collections import Counter\n",
        "\n",
        "# Select relevant columns\n",
        "selected_columns = [\"Sender\", \"Subject\", \"`Spam Indicator`\"]\n",
        "df = df.select(*selected_columns)\n",
        "\n",
        "df.show(5)\n",
        "\n",
        "# Check for null values in \"Subject,\" \"Sender,\" and \"Spam Indicator\" columns\n",
        "null_check_columns = [\"Subject\", \"Sender\", \"`Spam Indicator`\"]\n",
        "for column in null_check_columns:\n",
        "    null_count = df.filter(col(column).isNull()).count()\n",
        "    print(f\"Number of null values in {column}: {null_count}\")\n",
        "\n",
        "# Convert \"Subject\" to lowercase\n",
        "df = df.withColumn(\"Subject\", lower(col(\"Subject\")))\n",
        "\n",
        "# Handle null values in \"Spam Indicator\" column (assuming 0 for null)\n",
        "df = df.withColumn(\"`Spam Indicator`\", when(col(\"`Spam Indicator`\").isNull(), 0).otherwise(col(\"`Spam Indicator`\")))\n",
        "\n",
        "# Tokenize the \"Subject\" column\n",
        "tokenizer = Tokenizer(inputCol=\"Subject\", outputCol=\"words\")\n",
        "df = tokenizer.transform(df)\n",
        "\n",
        "# Remove stop words from the tokenized words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "df = remover.transform(df)\n",
        "\n",
        "# Select only specified columns\n",
        "selected_columns = [\"Subject\", \"Sender\", \"`Spam Indicator`\", \"filtered\"]\n",
        "df = df.select(*selected_columns)\n",
        "\n",
        "# Show the updated DataFrame\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "id": "2oWV2W6jNsfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate values based on \"Spam Indicator\" (0 for ham, 1 for spam)\n",
        "ham_df = df.filter(\"`Spam Indicator` == 0\")\n",
        "spam_df = df.filter(\"`Spam Indicator` == 1\")\n",
        "\n",
        "# Collect unique words used in ham and spam\n",
        "ham_words = ham_df.select(\"filtered\").rdd.flatMap(lambda x: x[\"filtered\"]).distinct().collect()\n",
        "spam_words = spam_df.select(\"filtered\").rdd.flatMap(lambda x: x[\"filtered\"]).distinct().collect()\n",
        "\n",
        "# Display unique words in ham and spam\n",
        "print(\"Unique words in ham emails:\")\n",
        "print(ham_words)\n",
        "\n",
        "print(\"\\nUnique words in spam emails:\")\n",
        "print(spam_words)\n",
        "\n",
        "# Show the updated DataFrame\n",
        "ham_df.show(5)\n",
        "spam_df.show(5)\n"
      ],
      "metadata": {
        "id": "kuqXch4y3I1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all the filtered words used in ham emails\n",
        "ham_words = ham_df.select(\"filtered\").rdd.flatMap(lambda x: x[\"filtered\"]).collect()\n",
        "\n",
        "# Use Counter to count occurrences of each word in ham emails\n",
        "word_counts_ham = Counter(ham_words)\n",
        "\n",
        "# Convert the Counter to a DataFrame\n",
        "word_counts_ham_df = spark.createDataFrame(word_counts_ham.items(), [\"word\", \"count\"])\n",
        "\n",
        "# Show the top 10 ham words\n",
        "print(\"Top 10 Ham Words:\")\n",
        "word_counts_ham_df.orderBy(F.desc(\"count\")).show(10)"
      ],
      "metadata": {
        "id": "SmMozUBVREYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "# Select the top 10 ham words\n",
        "top_ham_words = word_counts_ham_df.orderBy(F.desc(\"count\")).limit(10).select(\"word\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Explode the array column to separate rows in ham emails\n",
        "ham_df_exploded = ham_df.select(\"Sender\", \"Subject\", explode(\"filtered\").alias(\"word\"))\n",
        "\n",
        "# Filter based on the top ham words\n",
        "top_ham_senders = (\n",
        "    ham_df_exploded\n",
        "    .filter(col(\"word\").isin(top_ham_words))\n",
        "    .groupBy(\"Sender\", \"Subject\")\n",
        "    .count()\n",
        "    .orderBy(F.desc(\"count\"))\n",
        "    .limit(10)\n",
        ")\n",
        "\n",
        "# Show the top 10 ham senders and subjects\n",
        "print(\"Top 10 ham Senders and Subjects:\")\n",
        "top_ham_senders.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "l8rX0O7g7ZPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all the filtered words used in spam emails\n",
        "spam_words = spam_df.select(\"filtered\").rdd.flatMap(lambda x: x[\"filtered\"]).collect()\n",
        "\n",
        "# Use Counter to count occurrences of each word in spam emails\n",
        "word_counts_spam = Counter(spam_words)\n",
        "\n",
        "# Convert the Counter to a DataFrame\n",
        "word_counts_spam_df = spark.createDataFrame(word_counts_spam.items(), [\"word\", \"count\"])\n",
        "\n",
        "# Show the top 10 spam words\n",
        "print(\"Top 10 Spam Words:\")\n",
        "word_counts_spam_df.orderBy(F.desc(\"count\")).show(10)"
      ],
      "metadata": {
        "id": "jxCpeurh6eWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "# Select the top 10 spam words\n",
        "top_spam_words = word_counts_spam_df.orderBy(F.desc(\"count\")).limit(10).select(\"word\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Explode the array column to separate rows in spam emails\n",
        "spam_df_exploded = spam_df.select(\"Sender\", \"Subject\", explode(\"filtered\").alias(\"word\"))\n",
        "\n",
        "# Filter based on the top spam words\n",
        "top_spam_senders = (\n",
        "    spam_df_exploded\n",
        "    .filter(col(\"word\").isin(top_spam_words))\n",
        "    .groupBy(\"Sender\", \"Subject\")\n",
        "    .count()\n",
        "    .orderBy(F.desc(\"count\"))\n",
        "    .limit(10)\n",
        ")\n",
        "\n",
        "# Show the top 10 spam senders and subjects\n",
        "print(\"Top 10 Spam Senders and Subjects:\")\n",
        "top_spam_senders.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "qWKjuclL7n1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant columns from top_ham_senders\n",
        "df = top_ham_senders.select(\"Sender\", \"Subject\")\n",
        "\n",
        "# Tokenize the \"Subject\" column\n",
        "tokenizer_udf = udf(lambda text: text.split(), ArrayType(StringType()))\n",
        "df = df.withColumn(\"words\", tokenizer_udf(df[\"Subject\"]))\n",
        "\n",
        "# Calculate Term Frequencies (TF)\n",
        "calculate_tf_udf = udf(lambda word_list: {word: word_list.count(word) / len(word_list) for word in set(word_list)}, StringType())\n",
        "df = df.withColumn(\"tf\", calculate_tf_udf(df[\"words\"]))\n",
        "\n",
        "# Extract unique words\n",
        "unique_words = list(set(df.selectExpr(\"explode(words) as word\").select(\"word\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
        "\n",
        "# Calculate Inverse Document Frequencies (IDF)\n",
        "total_documents = df.count()\n",
        "document_frequency = df.select(\"Sender\", \"words\").rdd.flatMap(lambda x: [(word, 1) for word in set(x[1])]).reduceByKey(lambda x, y: x + y)\n",
        "idf_values = document_frequency.map(lambda x: (x[0], log(total_documents / x[1])))\n",
        "\n",
        "# Broadcast IDF values\n",
        "idf_broadcast = spark.sparkContext.broadcast(dict(idf_values.collect()))\n",
        "\n",
        "# Calculate TF-IDF for each document\n",
        "def calculate_tfidf(row):\n",
        "    user_name, words = row\n",
        "    tfidf_values = {word: words.count(word) * idf_broadcast.value.get(word, 0.0) for word in words}\n",
        "    return user_name, words, tfidf_values\n",
        "\n",
        "tfidf_data = df.select(\"Sender\", \"words\").rdd.map(calculate_tfidf)\n",
        "\n",
        "# Display the result\n",
        "tfidf_df = spark.createDataFrame(tfidf_data, [\"Sender\", \"words\", \"tfidf\"])\n",
        "tfidf_df.show(10, truncate = False)\n"
      ],
      "metadata": {
        "id": "QbIpuFKZZxQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant columns from top_spam_senders\n",
        "df = top_spam_senders.select(\"Sender\", \"Subject\")\n",
        "\n",
        "# Tokenize the \"Subject\" column\n",
        "tokenizer_udf = udf(lambda text: text.split(), ArrayType(StringType()))\n",
        "df = df.withColumn(\"words\", tokenizer_udf(df[\"Subject\"]))\n",
        "\n",
        "# Calculate Term Frequencies (TF)\n",
        "calculate_tf_udf = udf(lambda word_list: {word: word_list.count(word) / len(word_list) for word in set(word_list)}, StringType())\n",
        "df = df.withColumn(\"tf\", calculate_tf_udf(df[\"words\"]))\n",
        "\n",
        "# Extract unique words\n",
        "unique_words = list(set(df.selectExpr(\"explode(words) as word\").select(\"word\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
        "\n",
        "# Calculate Inverse Document Frequencies (IDF)\n",
        "total_documents = df.count()\n",
        "document_frequency = df.select(\"Sender\", \"words\").rdd.flatMap(lambda x: [(word, 1) for word in set(x[1])]).reduceByKey(lambda x, y: x + y)\n",
        "idf_values = document_frequency.map(lambda x: (x[0], log(total_documents / x[1])))\n",
        "\n",
        "# Broadcast IDF values\n",
        "idf_broadcast = spark.sparkContext.broadcast(dict(idf_values.collect()))\n",
        "\n",
        "# Calculate TF-IDF for each document\n",
        "def calculate_tfidf(row):\n",
        "    user_name, words = row\n",
        "    tfidf_values = {word: words.count(word) * idf_broadcast.value.get(word, 0.0) for word in words}\n",
        "    return user_name, words, tfidf_values\n",
        "\n",
        "tfidf_data = df.select(\"Sender\", \"words\").rdd.map(calculate_tfidf)\n",
        "\n",
        "# Display the result\n",
        "tfidf_df = spark.createDataFrame(tfidf_data, [\"Sender\", \"words\", \"tfidf\"])\n",
        "tfidf_df.show(10, truncate = False)\n",
        "\n"
      ],
      "metadata": {
        "id": "mnf-uT0_2LSa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}